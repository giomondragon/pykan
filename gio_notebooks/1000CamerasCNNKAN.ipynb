{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dea99dd-13b0-4b9f-8497-4b83f72afc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the root directory\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Add the root directory to the Python path\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5dcb076-7843-40fe-a0d5-850312d9f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from TINTOlib.tinto import TINTO\n",
    "from vit_pytorch.vit import ViT\n",
    "from kan import *\n",
    "\n",
    "\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import traceback\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a604e69-efd9-4720-8de5-0da057312df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4af556-2b79-408e-b782-9f45c1860ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 381\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317dfa67-934c-48f1-a237-69f908076e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"data/1000-Cameras-Dataset\"\n",
    "x_col=[\"Release_date\", \"Max_resolution\", \"Low_resolution\", \"Effective_pixels\", \"Zoom\", \"Normal_focus_range\", \"Macro_focus_range\", \"Storage_included\",\"Weight\",\"Dimensions\"]\n",
    "target_col=[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279ac1c3-cd38-494f-8fcd-2b02e482bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_impute(npy_filename, y_filename, x_col, target_col, folder):\n",
    "    \"\"\"\n",
    "    Load the feature array (npy_filename) and label array (y_filename),\n",
    "    compute the mean for each numerical column, and replace NaN values\n",
    "    with the column mean for numerical columns and '_new_' for non-numerical.\n",
    "    \"\"\"\n",
    "    # Load numpy arrays\n",
    "    X = np.load(os.path.join(folder, npy_filename))\n",
    "    y = np.load(os.path.join(folder, y_filename))\n",
    "    \n",
    "    # Ensure the number of rows matches between X and y\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(f\"The number of rows in {npy_filename} and {y_filename} do not match.\")\n",
    "    \n",
    "    # Convert arrays to DataFrames\n",
    "    df_X = pd.DataFrame(X, columns=x_col)\n",
    "    df_y = pd.DataFrame(y, columns=target_col)\n",
    "\n",
    "    # Compute mean values for numerical columns\n",
    "    numerical_means = df_X.select_dtypes(include=[np.number]).mean()\n",
    "\n",
    "    # Replace NaNs: numerical with mean, non-numerical with '_new_'\n",
    "    df_X = df_X.apply(lambda col: col.fillna(numerical_means[col.name]) if col.name in numerical_means else col.fillna('_new_'))\n",
    "\n",
    "    return df_X, df_y\n",
    "\n",
    "X_train, y_train = load_and_impute('N_train.npy', 'y_train.npy', x_col, target_col, folder)\n",
    "X_test, y_test   = load_and_impute('N_test.npy',  'y_test.npy', x_col, target_col, folder)\n",
    "X_val, y_val     = load_and_impute('N_val.npy',   'y_val.npy', x_col, target_col, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f90af460-8f7b-4cc9-aa1c-0799bab6ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train, y_train], axis=1)\n",
    "X_test = pd.concat([X_test, y_test], axis=1)\n",
    "X_val = pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0cbb5b-9cfc-4e39-bbe9-3e526ada260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(X_train, X_test, X_val, images_folder, image_model, problem_type, batch_size=32, device='cuda'):\n",
    "    \"\"\"Loads, processes, and converts a dataset into PyTorch DataLoaders for a ViT + KAN model.\"\"\"\n",
    "    \n",
    "    # Reset indices\n",
    "    data_splits = {\n",
    "        \"train\": X_train.reset_index(drop=True), \n",
    "        \"val\": X_val.reset_index(drop=True), \n",
    "        \"test\": X_test.reset_index(drop=True)\n",
    "    }\n",
    "    \n",
    "    # Process images and numerical features\n",
    "    num_features, img_paths, targets = {}, {}, {}\n",
    "\n",
    "    for split, X_split in data_splits.items():\n",
    "        split_folder = f\"{images_folder}/{split}\"\n",
    "        os.makedirs(split_folder, exist_ok=True)\n",
    "\n",
    "        # Ensure images and CSV files are generated\n",
    "        image_model.fit_transform(X_split, split_folder) if split == \"train\" else image_model.transform(X_split, split_folder)\n",
    "\n",
    "        # Validate CSV file existence\n",
    "        csv_path = os.path.join(split_folder, f\"{problem_type}.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"Error: Missing CSV at {csv_path}. Available files: {os.listdir(split_folder)}\")\n",
    "            raise FileNotFoundError(f\"Expected file {csv_path} not found. Ensure image_model generates it.\")\n",
    "\n",
    "        print(f\"Loading {csv_path}...\")\n",
    "        img_df = pd.read_csv(csv_path)\n",
    "        img_df[\"images\"] = split_folder + \"/\" + img_df[\"images\"]\n",
    "\n",
    "        # Combine numerical data and images\n",
    "        combined_df = pd.concat([img_df, X_split], axis=1)\n",
    "        num_features[split] = combined_df.drop(columns=[\"values\", \"images\", X_train.columns[-1]])\n",
    "        img_paths[split] = img_df[\"images\"]\n",
    "        targets[split] = combined_df[\"values\"]\n",
    "\n",
    "    # Standardize numerical data\n",
    "    scaler = StandardScaler()\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        num_features[split] = pd.DataFrame(scaler.fit_transform(num_features[split]) if split == \"train\" \n",
    "                                           else scaler.transform(num_features[split]), \n",
    "                                           columns=num_features[split].columns)\n",
    "\n",
    "    # Convert images to tensors\n",
    "    img_tensors = {split: torch.tensor(np.array([cv2.imread(img) for img in img_paths[split]]), \n",
    "                                       dtype=torch.float32, device=device).permute(0, 3, 1, 2) / 255.0\n",
    "                   for split in [\"train\", \"val\", \"test\"]}\n",
    "    \n",
    "    # Get dataset properties\n",
    "    attributes = num_features[\"train\"].shape[1]\n",
    "    channels, height, width = img_tensors[\"train\"].shape[1:]\n",
    "    imgs_shape = (channels, height, width)\n",
    "\n",
    "    print(\"Images shape:\", imgs_shape)\n",
    "    print(\"Attributes:\", attributes)\n",
    "\n",
    "    # Convert numerical data and targets to tensors\n",
    "    num_tensors = {split: torch.tensor(num_features[split].values, dtype=torch.float32, device=device)\n",
    "                   for split in [\"train\", \"val\", \"test\"]}\n",
    "    \n",
    "    target_tensors = {split: torch.tensor(targets[split].values, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "                      for split in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "    # Create DataLoaders\n",
    "    datasets = {split: TensorDataset(num_tensors[split], img_tensors[split], target_tensors[split]) \n",
    "                for split in [\"train\", \"val\", \"test\"]}\n",
    "    \n",
    "    data_loaders = {split: DataLoader(datasets[split], batch_size=batch_size, shuffle=(split == \"train\")) \n",
    "                    for split in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "    return data_loaders[\"train\"], data_loaders[\"val\"], data_loaders[\"test\"], attributes, imgs_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bcdf627-a811-49bf-9041-27290e55764e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = X_train.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate the square root for image size\n",
    "import math\n",
    "image_size = math.ceil(math.sqrt(columns_minus_one))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd4dc1e6-4d92-4799-9689-253e52ee70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TINTOlib.igtd import IGTD\n",
    "dataset_name = '1000cameras'\n",
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], fea_dist_method='Euclidean', image_dist_method='Euclidean', error='abs', max_step=30000, val_step=300, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size}x{image_size}_fEuclidean_iEuclidean_abs\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81e4d4a5-6d27-486a-adb1-040ea011edf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HyNNImages/Regression/1000cameras/images_1000cameras_IGTD_4x4_fEuclidean_iEuclidean_abs/train\\regression.csv...\n",
      "Loading HyNNImages/Regression/1000cameras/images_1000cameras_IGTD_4x4_fEuclidean_iEuclidean_abs/val\\regression.csv...\n",
      "Loading HyNNImages/Regression/1000cameras/images_1000cameras_IGTD_4x4_fEuclidean_iEuclidean_abs/test\\regression.csv...\n",
      "Images shape: (3, 4, 4)\n",
      "Attributes: 10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAQAAAAECAYAAACp8Z5+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAAAnAAAAJwEqCZFPAAAAOElEQVR4nE3BQQ3AQAhFwd9awBM+8IDJFUNCEMDrqcnOqLsBcHfMjHdmFBHaXWWmVFWcc/g9ALp8uMcmO+o1YbAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 4x4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAQAAAAECAYAAACp8Z5+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAAAnAAAAJwEqCZFPAAAANklEQVR4nE3BsQ0AMQgEwXPntEYH5ESGHhDZOnrpZ5SZuDsAZsbpbmZGEaGqknaXey+fA6CfB9hwLG1iA8PLAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 4x4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAQAAAAECAYAAACp8Z5+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAAAnAAAAJwEqCZFPAAAAOUlEQVR4nE3BwQnAQAhFwbeQTizDmuzXEhY8q8efUyAzZ2YUEQDce3m6GzOjqnB32F1lpj5Hkvh5AcurIgB6TU0tAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 4x4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape = load_and_preprocess_data(\n",
    "    X_train, X_test, X_val,\n",
    "    images_folder=images_folder,\n",
    "    image_model=image_model,\n",
    "    problem_type=problem_type,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "106dd90e-1a2e-4257-9ac8-af8840b232fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, device):\n",
    "        super(Model1, self).__init__()\n",
    "        self.device = device\n",
    "        # KAN branch\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, 21],\n",
    "            grid=5,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "        \n",
    "        # Calculate the size of the flattened CNN output\n",
    "        self.cnn_output_size = 16 * (imgs_shape[1] - 1) * (imgs_shape[2] - 1)\n",
    "        \n",
    "        # Final MLP layers\n",
    "        self.final_kan = KAN(\n",
    "            width=[self.cnn_output_size + 21, 1],\n",
    "            grid=3,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        # Ensure inputs are moved to the correct device\n",
    "        kan_input = mlp_input.to(self.device)\n",
    "        cnn_input = cnn_input.to(self.device)\n",
    "        \n",
    "        cnn_output = self.cnn_branch(cnn_input)  # Process image input\n",
    "        kan_output = self.m_kan(kan_input)  # Process numerical input\n",
    "        \n",
    "        concat_output = torch.cat((kan_output, cnn_output), dim=1)\n",
    "        return self.final_kan(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef62d61a-3d55-48e9-8098-5a49cc9f4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_create_model(model_class, attributes, imgs_shape, device):\n",
    "    try:\n",
    "        model = model_class(attributes, imgs_shape, device)\n",
    "        \n",
    "        # Test the model with a sample input\n",
    "        num_input = torch.randn(4, attributes)\n",
    "        img_input = torch.randn(4, *imgs_shape)\n",
    "        output = model(num_input, img_input)\n",
    "        \n",
    "        print(f\"Successfully created and tested {model_class.__name__}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or testing {model_class.__name__}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae16e667-1905-4eca-bcc6-7934be63846d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model1\n"
     ]
    }
   ],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape, device)  # Attempt to create Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c8cb293-6607-43e3-9407-55bc1e5b3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for num_data, img_data, targets in data_loader:\n",
    "            num_data, img_data, targets = num_data.to(device, non_blocking=True), img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(num_data, img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "    mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_predictions)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a96c15-6dd2-4147-87f6-b2e4362db219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit_2(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=32, epochs=10, min_lr=1e-3, max_lr=1, device='cuda', weight_decay=1e-2):\n",
    "    model = model\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=max_lr/min_lr, total_steps=total_steps, pct_start=0.3, final_div_factor=1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mse': [], 'val_mse': [], 'train_rmse': [], 'val_rmse': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    start_time = time.time()\n",
    "    pbar = tqdm(range(epochs), desc='description', ncols=100)\n",
    "    #for epoch in range(epochs):\n",
    "    for epoch in pbar:\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "        for num_data, img_data, targets in train_loader:\n",
    "            num_data, img_data, targets = num_data.to(device, non_blocking=True), img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(num_data, img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for num_data, img_data, targets in val_loader:\n",
    "                num_data, img_data, targets = num_data.to(device, non_blocking=True), img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                outputs = model(num_data, img_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "        \n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "        train_mse = mean_squared_error(train_targets, train_predictions)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        val_mse = mean_squared_error(val_targets, val_predictions)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        train_r2 = r2_score(train_targets, train_predictions)\n",
    "        val_r2 = r2_score(val_targets, val_predictions)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "        history['val_mse'].append(val_mse)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        pbar.set_description(\"| train_rmse: %.2e | val_rmse: %.2e | \" % (train_rmse, val_rmse))\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    train_metrics = calculate_metrics(model, train_loader, device)\n",
    "    val_metrics = calculate_metrics(model, val_loader, device)\n",
    "    test_metrics = calculate_metrics(model, test_loader, device)\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'train_mse': train_metrics['mse'],\n",
    "        'train_mae': train_metrics['mae'],\n",
    "        'train_rmse': train_metrics['rmse'],\n",
    "        'train_r2': train_metrics['r2'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_mse': val_metrics['mse'],\n",
    "        'val_mae': val_metrics['mae'],\n",
    "        'val_rmse': val_metrics['rmse'],\n",
    "        'val_r2': val_metrics['r2'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_mse': test_metrics['mse'],\n",
    "        'test_mae': test_metrics['mae'],\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'test_r2': test_metrics['r2'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "    print(f\"Best model found at epoch {best_epoch}/{epochs}\")\n",
    "    print(f\"Best Train Loss: {history['train_loss'][best_epoch-1]:.4f}, Best Val Loss: {history['val_loss'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train MSE: {history['train_mse'][best_epoch-1]:.4f}, Best Val MSE: {history['val_mse'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train RMSE: {history['train_rmse'][best_epoch-1]:.4f}, Best Val RMSE: {history['val_rmse'][best_epoch-1]:.4f}\")\n",
    "\n",
    "    # Save metrics to a file\n",
    "    os.makedirs(f'logs/Regression/{dataset_name}/ViT+MLP/{model_name}', exist_ok=True)\n",
    "    with open(f'logs/Regression/{dataset_name}/ViT+MLP/{model_name}/metrics.txt', 'w') as f:\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "            \n",
    "    # Save best model\n",
    "    model_save_path = f\"models/Regression/{dataset_name}/ViT+MLP/{model_name}/best_model.pth\"\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(best_model, model_save_path)\n",
    "    print(f\"Best model saved to {model_save_path}\")\n",
    "            \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7275d913-516a-4c5d-8da9-5854e4ea6c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_rmse: 6.75e+02 | val_rmse: 7.36e+02 | : 100%|███████████████| 20/20 [03:15<00:00,  9.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 195.07 seconds\n",
      "Best model found at epoch 20/20\n",
      "Best Train Loss: 481264.3165, Best Val Loss: 513299.3203\n",
      "Best Train MSE: 455279.8438, Best Val MSE: 542020.5000\n",
      "Best Train RMSE: 674.7443, Best Val RMSE: 736.2204\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model1/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "metrics = compile_and_fit_2(model1, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model1\", device='cuda',\n",
    "                            batch_size=32, epochs=20, min_lr=1e-4, max_lr=4e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f68f7797-2373-41e6-9ed7-a4788f7c4f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_rmse: 6.17e+02 | val_rmse: 6.96e+02 | : 100%|███████████████| 10/10 [01:38<00:00,  9.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 98.40 seconds\n",
      "Best model found at epoch 1/10\n",
      "Best Train Loss: 174634.5623, Best Val Loss: 392041.5284\n",
      "Best Train MSE: 174648.0156, Best Val MSE: 414289.5000\n",
      "Best Train RMSE: 417.9091, Best Val RMSE: 643.6533\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model1/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "metrics = compile_and_fit_2(model1, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model1\", device='cuda',\n",
    "                            batch_size=32, epochs=10, min_lr=1e-2, max_lr=1, weight_decay=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a308cc2d-ee41-41d8-bad3-54cbddb40c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 161 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1.final_kan.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9cc7155-a53f-4398-82db-80ecc00fa4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_rmse: 3.37e+02 | val_rmse: 6.42e+02 | : 100%|███████████████| 40/40 [05:27<00:00,  8.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 327.12 seconds\n",
      "Best model found at epoch 1/40\n",
      "Best Train Loss: 134095.4410, Best Val Loss: 341094.8945\n",
      "Best Train MSE: 134931.4219, Best Val MSE: 360080.3438\n",
      "Best Train RMSE: 367.3301, Best Val RMSE: 600.0670\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model1/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "metrics = compile_and_fit_2(model1, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model1\", batch_size=32, epochs=40, min_lr=1e-4, max_lr=4e-1, device='cuda', weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08b7c51a-bf8f-444b-864c-d4b4c24ae291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import LBFGS\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def combine_loader(loader):\n",
    "    \"\"\"\n",
    "    Combines all batches from a DataLoader into three tensors.\n",
    "    Assumes each batch is a tuple: (mlp_tensor, img_tensor, target_tensor)\n",
    "    \"\"\"\n",
    "    mlp_list, img_list, target_list = [], [], []\n",
    "    for mlp, img, target in loader:\n",
    "        mlp_list.append(mlp)\n",
    "        img_list.append(img)\n",
    "        target_list.append(target)\n",
    "    return torch.cat(mlp_list, dim=0), torch.cat(img_list, dim=0), torch.cat(target_list, dim=0)\n",
    "\n",
    "def fit_hybrid_dataloaders(model, \n",
    "                           train_loader, \n",
    "                           val_loader, \n",
    "                           test_loader, \n",
    "                           opt=\"Adam\", \n",
    "                           steps=100, \n",
    "                           log=1, \n",
    "                           lamb=0., \n",
    "                           lamb_l1=1., \n",
    "                           lamb_entropy=2., \n",
    "                           lamb_coef=0., \n",
    "                           lamb_coefdiff=0., \n",
    "                           update_grid=True, \n",
    "                           grid_update_num=10, \n",
    "                           loss_fn=None, \n",
    "                           lr=1., \n",
    "                           start_grid_update_step=-1, \n",
    "                           stop_grid_update_step=50, \n",
    "                           batch=-1,\n",
    "                           metrics=None, \n",
    "                           save_fig=False, \n",
    "                           in_vars=None, \n",
    "                           out_vars=None, \n",
    "                           beta=3, \n",
    "                           save_fig_freq=1, \n",
    "                           img_folder='./video', \n",
    "                           singularity_avoiding=False, \n",
    "                           y_th=1000., \n",
    "                           reg_metric='edge_forward_spline_n', \n",
    "                           display_metrics=None):\n",
    "    \"\"\"\n",
    "    Trains the hybrid model (with a KAN branch and a CNN branch) using a steps-based loop\n",
    "    adapted from KAN.fit(), with grid updates and regularization.\n",
    "    \n",
    "    Instead of a single dataset dict, this function accepts three DataLoaders:\n",
    "        - train_loader: provides (mlp, img, target) for training\n",
    "        - val_loader: provides (mlp, img, target) for validation\n",
    "        - test_loader: provides (mlp, img, target) for evaluation during training\n",
    "\n",
    "    Internally, the function combines each loader into a dataset dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        results: dictionary containing training loss, evaluation loss, regularization values,\n",
    "                 and any additional metrics recorded during training.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Combine dataloaders into tensors.\n",
    "    train_mlp, train_img, train_target = combine_loader(train_loader)\n",
    "    val_mlp, val_img, val_target = combine_loader(val_loader)\n",
    "    test_mlp, test_img, test_target = combine_loader(test_loader)\n",
    "    \n",
    "    dataset = {\n",
    "        \"train_mlp\": train_mlp.to(device),\n",
    "        \"train_img\": train_img.to(device),\n",
    "        \"train_target\": train_target.to(device),\n",
    "        \"val_mlp\": val_mlp.to(device),\n",
    "        \"val_img\": val_img.to(device),\n",
    "        \"val_target\": val_target.to(device),\n",
    "        \"test_mlp\": test_mlp.to(device),\n",
    "        \"test_img\": test_img.to(device),\n",
    "        \"test_target\": test_target.to(device),\n",
    "    }\n",
    "\n",
    "    # Warn if regularization is requested but model's internal flag isn't enabled.\n",
    "    if lamb > 0. and not getattr(model.m_kan, \"save_act\", False):\n",
    "        print(\"setting lamb=0. If you want to set lamb > 0, set model.m_kan.save_act=True\")\n",
    "    \n",
    "    # Disable symbolic processing for training if applicable (KAN internal logic)\n",
    "    if hasattr(model.m_kan, \"disable_symbolic_in_fit\"):\n",
    "        old_save_act, old_symbolic_enabled = model.m_kan.disable_symbolic_in_fit(lamb)\n",
    "    else:\n",
    "        old_save_act, old_symbolic_enabled = None, None\n",
    "\n",
    "    pbar = tqdm(range(steps), desc='Training', ncols=100)\n",
    "\n",
    "    # Default loss function (mean squared error) if not provided\n",
    "    if loss_fn is None:\n",
    "        loss_fn = lambda x, y: torch.mean((x - y) ** 2)\n",
    "\n",
    "    # Determine grid update frequency\n",
    "    grid_update_freq = int(stop_grid_update_step / grid_update_num) if grid_update_num > 0 else 1\n",
    "\n",
    "    # Determine total number of training examples\n",
    "    n_train = dataset[\"train_mlp\"].shape[0]\n",
    "    n_eval  = dataset[\"test_mlp\"].shape[0]  # using test set for evaluation during training\n",
    "    batch_size = n_train if batch == -1 or batch > n_train else batch\n",
    "\n",
    "    # Set up optimizer: choose between Adam and LBFGS (removed tolerance_ys)\n",
    "    if opt == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif opt == \"LBFGS\":\n",
    "        optimizer = LBFGS(model.parameters(), lr=lr, history_size=10, \n",
    "                          line_search_fn=\"strong_wolfe\", \n",
    "                          tolerance_grad=1e-32, \n",
    "                          tolerance_change=1e-32)\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer not recognized. Use 'Adam' or 'LBFGS'.\")\n",
    "\n",
    "    # Prepare results dictionary.\n",
    "    results = {\n",
    "        'train_loss': [],\n",
    "        'eval_loss': [],\n",
    "        'reg': []\n",
    "    }\n",
    "    if metrics is not None:\n",
    "        for metric in metrics:\n",
    "            results[metric.__name__] = []\n",
    "\n",
    "    for step in pbar:\n",
    "        # Randomly sample indices for a mini-batch from the training set.\n",
    "        train_indices = np.random.choice(n_train, batch_size, replace=False)\n",
    "        # Use full evaluation set for evaluation; you can also sample if desired.\n",
    "        eval_indices = np.arange(n_eval)\n",
    "\n",
    "        # Closure for LBFGS\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            mlp_batch = dataset[\"train_mlp\"][train_indices]\n",
    "            img_batch = dataset[\"train_img\"][train_indices]\n",
    "            target_batch = dataset[\"train_target\"][train_indices]\n",
    "            outputs = model(mlp_batch, img_batch)\n",
    "            train_loss = loss_fn(outputs, target_batch)\n",
    "            # Compute regularization term if enabled.\n",
    "            if hasattr(model.m_kan, \"save_act\") and model.m_kan.save_act:\n",
    "                if reg_metric == 'edge_backward':\n",
    "                    model.m_kan.attribute()\n",
    "                if reg_metric == 'node_backward':\n",
    "                    model.m_kan.node_attribute()\n",
    "                reg_val_inner = model.m_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            else:\n",
    "                reg_val_inner = torch.tensor(0., device=device)\n",
    "            loss_val_inner = train_loss + lamb * reg_val_inner\n",
    "            loss_val_inner.backward()\n",
    "            return loss_val_inner\n",
    "\n",
    "        # Perform grid update if applicable.\n",
    "        if (step % grid_update_freq == 0 and step < stop_grid_update_step \n",
    "            and update_grid and step >= start_grid_update_step):\n",
    "            model.m_kan.update_grid(dataset[\"train_mlp\"][train_indices])\n",
    "\n",
    "        # Perform an optimizer step.\n",
    "        if opt == \"LBFGS\":\n",
    "            optimizer.step(closure)\n",
    "            loss_val = closure()  # re-evaluate to record loss\n",
    "            # Compute reg_val separately for logging\n",
    "            with torch.no_grad():\n",
    "                mlp_batch = dataset[\"train_mlp\"][train_indices]\n",
    "                img_batch = dataset[\"train_img\"][train_indices]\n",
    "                target_batch = dataset[\"train_target\"][train_indices]\n",
    "                outputs = model(mlp_batch, img_batch)\n",
    "                train_loss = loss_fn(outputs, target_batch)\n",
    "                if hasattr(model.m_kan, \"save_act\") and model.m_kan.save_act:\n",
    "                    if reg_metric == 'edge_backward':\n",
    "                        model.m_kan.attribute()\n",
    "                    if reg_metric == 'node_backward':\n",
    "                        model.m_kan.node_attribute()\n",
    "                    reg_val = model.m_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "                else:\n",
    "                    reg_val = torch.tensor(0., device=device)\n",
    "        else:  # Adam branch\n",
    "            optimizer.zero_grad()\n",
    "            mlp_batch = dataset[\"train_mlp\"][train_indices]\n",
    "            img_batch = dataset[\"train_img\"][train_indices]\n",
    "            target_batch = dataset[\"train_target\"][train_indices]\n",
    "            outputs = model(mlp_batch, img_batch)\n",
    "            train_loss = loss_fn(outputs, target_batch)\n",
    "            if hasattr(model.m_kan, \"save_act\") and model.m_kan.save_act:\n",
    "                if reg_metric == 'edge_backward':\n",
    "                    model.m_kan.attribute()\n",
    "                if reg_metric == 'node_backward':\n",
    "                    model.m_kan.node_attribute()\n",
    "                reg_val = model.m_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            else:\n",
    "                reg_val = torch.tensor(0., device=device)\n",
    "            loss_val = train_loss + lamb * reg_val\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate on the entire evaluation set (from test_loader)\n",
    "        mlp_eval = dataset[\"test_mlp\"][eval_indices]\n",
    "        img_eval = dataset[\"test_img\"][eval_indices]\n",
    "        target_eval = dataset[\"test_target\"][eval_indices]\n",
    "        eval_loss = loss_fn(model(mlp_eval, img_eval), target_eval)\n",
    "\n",
    "        # Record results (using square-root of loss similar to KAN.fit)\n",
    "        results['train_loss'].append(torch.sqrt(loss_val.detach()).item())\n",
    "        results['eval_loss'].append(torch.sqrt(eval_loss.detach()).item())\n",
    "        results['reg'].append(reg_val.detach().item())\n",
    "\n",
    "        if metrics is not None:\n",
    "            for metric in metrics:\n",
    "                # Here, we assume each metric returns a tensor.\n",
    "                results[metric.__name__].append(metric().item())\n",
    "\n",
    "        # Update progress bar.\n",
    "        if display_metrics is None:\n",
    "            pbar.set_description(\"| train_loss: %.2e | eval_loss: %.2e | reg: %.2e |\" %\n",
    "                                 (torch.sqrt(loss_val.detach()).item(),\n",
    "                                  torch.sqrt(eval_loss.detach()).item(),\n",
    "                                  reg_val.detach().item()))\n",
    "        else:\n",
    "            desc = \"\"\n",
    "            data = []\n",
    "            for metric in display_metrics:\n",
    "                desc += f\" {metric}: %.2e |\"\n",
    "                data.append(results[metric.__name__][-1])\n",
    "            pbar.set_description(desc % tuple(data))\n",
    "\n",
    "        # Optionally save a figure snapshot.\n",
    "        if save_fig and step % save_fig_freq == 0:\n",
    "            save_act_backup = getattr(model.m_kan, \"save_act\", False)\n",
    "            model.m_kan.save_act = True\n",
    "            model.plot(folder=img_folder, in_vars=in_vars, out_vars=out_vars, title=f\"Step {step}\", beta=beta)\n",
    "            plt.savefig(os.path.join(img_folder, f\"{step}.jpg\"), bbox_inches='tight', dpi=200)\n",
    "            plt.close()\n",
    "            model.m_kan.save_act = save_act_backup\n",
    "\n",
    "    # Restore original settings if applicable.\n",
    "    if old_symbolic_enabled is not None:\n",
    "        model.m_kan.symbolic_enabled = old_symbolic_enabled\n",
    "    if hasattr(model.m_kan, \"log_history\"):\n",
    "        model.m_kan.log_history('fit')\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7145ac01-a49d-4b64-8149-484d968dcb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                                                              | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "| train_loss: 5.84e+02 | eval_loss: 6.64e+02 | reg: 0.00e+00 |:   0%|        | 0/10 [00:01<?, ?it/s]\u001b[A\n",
      "| train_loss: 5.84e+02 | eval_loss: 6.64e+02 | reg: 0.00e+00 |:  10%| | 1/10 [00:01<00:09,  1.08s/it\u001b[A\n",
      "| train_loss: 7.05e+02 | eval_loss: 7.10e+02 | reg: 0.00e+00 |:  10%| | 1/10 [00:02<00:09,  1.08s/it\u001b[A\n",
      "| train_loss: 7.05e+02 | eval_loss: 7.10e+02 | reg: 0.00e+00 |:  20%|▏| 2/10 [00:02<00:08,  1.05s/it\u001b[A\n",
      "| train_loss: 6.95e+02 | eval_loss: 6.56e+02 | reg: 0.00e+00 |:  20%|▏| 2/10 [00:03<00:08,  1.05s/it\u001b[A\n",
      "| train_loss: 6.95e+02 | eval_loss: 6.56e+02 | reg: 0.00e+00 |:  30%|▎| 3/10 [00:03<00:07,  1.08s/it\u001b[A\n",
      "| train_loss: 6.02e+02 | eval_loss: 6.66e+02 | reg: 0.00e+00 |:  30%|▎| 3/10 [00:04<00:07,  1.08s/it\u001b[A\n",
      "| train_loss: 6.02e+02 | eval_loss: 6.66e+02 | reg: 0.00e+00 |:  40%|▍| 4/10 [00:04<00:06,  1.09s/it\u001b[A\n",
      "| train_loss: 6.15e+02 | eval_loss: 5.83e+02 | reg: 0.00e+00 |:  40%|▍| 4/10 [00:05<00:06,  1.09s/it\u001b[A\n",
      "| train_loss: 6.15e+02 | eval_loss: 5.83e+02 | reg: 0.00e+00 |:  50%|▌| 5/10 [00:05<00:05,  1.05s/it\u001b[A\n",
      "| train_loss: 5.45e+02 | eval_loss: 5.36e+02 | reg: 0.00e+00 |:  50%|▌| 5/10 [00:06<00:05,  1.05s/it\u001b[A\n",
      "| train_loss: 5.45e+02 | eval_loss: 5.36e+02 | reg: 0.00e+00 |:  60%|▌| 6/10 [00:06<00:04,  1.08s/it\u001b[A\n",
      "| train_loss: 5.19e+02 | eval_loss: 4.85e+02 | reg: 0.00e+00 |:  60%|▌| 6/10 [00:07<00:04,  1.08s/it\u001b[A\n",
      "| train_loss: 5.19e+02 | eval_loss: 4.85e+02 | reg: 0.00e+00 |:  70%|▋| 7/10 [00:07<00:03,  1.08s/it\u001b[A\n",
      "| train_loss: 4.83e+02 | eval_loss: 4.98e+02 | reg: 0.00e+00 |:  70%|▋| 7/10 [00:08<00:03,  1.08s/it\u001b[A\n",
      "| train_loss: 4.83e+02 | eval_loss: 4.98e+02 | reg: 0.00e+00 |:  80%|▊| 8/10 [00:08<00:02,  1.08s/it\u001b[A\n",
      "| train_loss: 4.95e+02 | eval_loss: 4.97e+02 | reg: 0.00e+00 |:  80%|▊| 8/10 [00:09<00:02,  1.08s/it\u001b[A\n",
      "| train_loss: 4.95e+02 | eval_loss: 4.97e+02 | reg: 0.00e+00 |:  90%|▉| 9/10 [00:09<00:01,  1.07s/it\u001b[A\n",
      "| train_loss: 4.75e+02 | eval_loss: 4.94e+02 | reg: 0.00e+00 |:  90%|▉| 9/10 [00:10<00:01,  1.07s/it\u001b[A\n",
      "| train_loss: 4.75e+02 | eval_loss: 4.94e+02 | reg: 0.00e+00 |: 100%|█| 10/10 [00:10<00:00,  1.07s/i\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = fit_hybrid_dataloaders(model1, train_loader, val_loader, test_loader, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cff8beb-7490-463b-9c17-442bad618d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                              | 0/10 [07:04<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 167 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1.final_kan.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51a67e43-3ef1-4f77-82f1-13556d8359d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_rmse: 2.99e+02 | val_rmse: 6.32e+02 | : 100%|███████████████| 40/40 [04:32<00:00,  6.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 272.57 seconds\n",
      "Best model found at epoch 28/40\n",
      "Best Train Loss: 105591.4686, Best Val Loss: 364157.9268\n",
      "Best Train MSE: 106593.3984, Best Val MSE: 384182.4062\n",
      "Best Train RMSE: 326.4865, Best Val RMSE: 619.8245\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model1/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "metrics = compile_and_fit_2(model1, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model1\", batch_size=32, epochs=40, min_lr=1e-4, max_lr=4e-1, device='cuda', weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b8b220f-93fb-479c-82ce-4f06053df93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 156 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1.final_kan.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "230ddd2b-8be8-4d32-887e-df819e0d856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, device):\n",
    "        super(Model2, self).__init__()\n",
    "        self.device = device\n",
    "        # KAN branch\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, 8],\n",
    "            grid=3,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], out_channels=16, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, kernel_size=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "\n",
    "        # Calculate the size of the flattened output\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # Final MLP layers\n",
    "        self.final_kan = KAN(\n",
    "            width=[self.flat_size + 8, 1],\n",
    "            grid=3,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        # Forward pass with dummy input to calculate flat size\n",
    "        dummy_cnn_input = torch.zeros(1, *imgs_shape, device=device)\n",
    "        cnn_output = self.cnn_branch(dummy_cnn_input)\n",
    "        \n",
    "        return cnn_output.size(1)\n",
    "\n",
    "\n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        # Ensure inputs are moved to the correct device\n",
    "        kan_input = mlp_input.to(self.device)\n",
    "        cnn_input = cnn_input.to(self.device)\n",
    "        \n",
    "        cnn_output = self.cnn_branch(cnn_input)  # Process image input\n",
    "        kan_output = self.m_kan(kan_input)  # Process numerical input\n",
    "        \n",
    "        concat_output = torch.cat((kan_output, cnn_output), dim=1)\n",
    "        return self.final_kan(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7166aed-7c7f-483c-9e31-a98836679692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model2\n"
     ]
    }
   ],
   "source": [
    "model2 = try_create_model(Model2, attributes, imgs_shape, device)  # Attempt to create Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a15f584-f6b2-4858-9d87-23e9f794ffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_rmse: 3.61e+02 | val_rmse: 6.39e+02 | : 100%|███████████████| 20/20 [02:42<00:00,  8.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 162.18 seconds\n",
      "Best model found at epoch 10/20\n",
      "Best Train Loss: 201451.6917, Best Val Loss: 362719.1428\n",
      "Best Train MSE: 202708.5625, Best Val MSE: 379645.7500\n",
      "Best Train RMSE: 450.2317, Best Val RMSE: 616.1540\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model2/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "metrics = compile_and_fit_2(model2, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model2\", device='cuda',\n",
    "                            batch_size=32, epochs=20, min_lr=1e-4, max_lr=4e-1, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b329c80d-d1af-457c-bf20-b98c9f59e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_rmse: 3.53e+02 | val_rmse: 6.43e+02 | : 100%|███████████████| 20/20 [02:27<00:00,  7.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 147.09 seconds\n",
      "Best model found at epoch 5/20\n",
      "Best Train Loss: 274424.1719, Best Val Loss: 388146.9885\n",
      "Best Train MSE: 277298.8438, Best Val MSE: 409868.3438\n",
      "Best Train RMSE: 526.5917, Best Val RMSE: 640.2096\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model2/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "metrics = compile_and_fit_2(model2, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model2\", device='cuda',\n",
    "                            batch_size=32, epochs=20, min_lr=1e-4, max_lr=4e-1, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aee99054-f096-41eb-b2ba-4017bbd30063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit_hybrid(model, train_loader, val_loader, test_loader,\n",
    "                           dataset_name, model_name,\n",
    "                           batch_size=32, epochs=10,\n",
    "                           min_lr=1e-4, max_lr=4e-3,\n",
    "                           device='cuda', weight_decay=1e-2,\n",
    "                           update_grid=True, grid_update_num=10,\n",
    "                           start_grid_update_step=0, stop_grid_update_step=50,\n",
    "                           lamb=0.0, lamb_l1=1.0, lamb_entropy=2.0, \n",
    "                           lamb_coef=0.0, lamb_coefdiff=0.0,\n",
    "                           singularity_avoiding=False, y_th=1000.,\n",
    "                           save_fig=False, save_fig_freq=1, img_folder='./video'):\n",
    "    \"\"\"\n",
    "    Trains a hybrid model (KAN branch + CNN branch) with features inspired by KAN.fit.\n",
    "    \n",
    "    Besides the usual MSE loss, this routine can update each KAN branch’s grid periodically and \n",
    "    add regularization penalties if enabled. It uses OneCycleLR for learning rate scheduling and saves \n",
    "    the best model (by validation loss).\n",
    "    \n",
    "    Returns:\n",
    "        metrics: dictionary containing losses, MSE, RMSE, R2 and timing information.\n",
    "    \"\"\"\n",
    "    import copy, os, time, gc\n",
    "    from tqdm import tqdm\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.optim.lr_scheduler import OneCycleLR\n",
    "    # Ensure loss_fn is MSE\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=(max_lr/min_lr),\n",
    "                            total_steps=total_steps, pct_start=0.3, final_div_factor=1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'learning_rate': [], 'epoch_time': []}\n",
    "    \n",
    "    # Determine grid update frequency (in steps)\n",
    "    grid_update_freq = total_steps // grid_update_num if update_grid else None\n",
    "    global_step = 0\n",
    "    overall_start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(range(epochs), desc='Epoch', ncols=100)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss_epoch = 0.0\n",
    "        \n",
    "        for batch_idx, (num_data, img_data, targets) in enumerate(train_loader):\n",
    "            num_data = num_data.to(device, non_blocking=True)\n",
    "            img_data = img_data.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform grid update if enabled and if within the scheduled steps\n",
    "            if update_grid and grid_update_freq is not None:\n",
    "                if (global_step % grid_update_freq == 0 and \n",
    "                    global_step >= start_grid_update_step and \n",
    "                    global_step < stop_grid_update_step):\n",
    "                    # Update the grid for the KAN branch that processes numerical features\n",
    "                    if hasattr(model.m_kan, 'update_grid'):\n",
    "                        model.m_kan.update_grid(num_data)\n",
    "                    # For the final KAN branch, update using the concatenated input.\n",
    "                    if hasattr(model.final_kan, 'update_grid'):\n",
    "                        # Do a forward pass through the first branches only (without final_kan)\n",
    "                        with torch.no_grad():\n",
    "                            kan_out = model.m_kan(num_data)\n",
    "                            cnn_out = model.cnn_branch(img_data)\n",
    "                            concat_in = torch.cat((kan_out, cnn_out), dim=1)\n",
    "                        model.final_kan.update_grid(concat_in)\n",
    "            \n",
    "            # Forward pass through the hybrid model\n",
    "            outputs = model(num_data, img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            # Optionally add regularization from the KAN branches (if enabled)\n",
    "            reg_loss = 0.0\n",
    "            if hasattr(model.m_kan, 'save_act') and model.m_kan.save_act:\n",
    "                reg_loss += model.m_kan.get_reg('edge_forward_spline_n', lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            if hasattr(model.final_kan, 'save_act') and model.final_kan.save_act:\n",
    "                reg_loss += model.final_kan.get_reg('edge_forward_spline_n', lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            \n",
    "            total_loss = loss + lamb * reg_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss_epoch += total_loss.item()\n",
    "            global_step += 1\n",
    "        \n",
    "        train_loss_epoch /= len(train_loader)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0.0\n",
    "        with torch.no_grad():\n",
    "            for num_data, img_data, targets in val_loader:\n",
    "                num_data = num_data.to(device, non_blocking=True)\n",
    "                img_data = img_data.to(device, non_blocking=True)\n",
    "                targets = targets.to(device, non_blocking=True)\n",
    "                outputs = model(num_data, img_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss_epoch += loss.item()\n",
    "        val_loss_epoch /= len(val_loader)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        history['train_loss'].append(train_loss_epoch)\n",
    "        history['val_loss'].append(val_loss_epoch)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        \n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{epochs} | train_loss: {train_loss_epoch:.2e} | val_loss: {val_loss_epoch:.2e}\")\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if val_loss_epoch < best_val_loss:\n",
    "            best_val_loss = val_loss_epoch\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "        \n",
    "        # Optionally, save model plots if your model supports it\n",
    "        if save_fig and (epoch + 1) % save_fig_freq == 0:\n",
    "            if hasattr(model, 'plot'):\n",
    "                model.plot(folder=img_folder, title=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "    total_time = time.time() - overall_start_time\n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "    print(f\"Best model found at epoch {best_epoch} with validation loss {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Calculate metrics on train, validation, and test sets\n",
    "    train_metrics = calculate_metrics(model, train_loader, device)\n",
    "    val_metrics = calculate_metrics(model, val_loader, device)\n",
    "    test_metrics = calculate_metrics(model, test_loader, device)\n",
    "    \n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'train_mse': train_metrics['mse'],\n",
    "        'train_mae': train_metrics['mae'],\n",
    "        'train_rmse': train_metrics['rmse'],\n",
    "        'train_r2': train_metrics['r2'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_mse': val_metrics['mse'],\n",
    "        'val_mae': val_metrics['mae'],\n",
    "        'val_rmse': val_metrics['rmse'],\n",
    "        'val_r2': val_metrics['r2'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_mse': test_metrics['mse'],\n",
    "        'test_mae': test_metrics['mae'],\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'test_r2': test_metrics['r2'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "    \n",
    "    # Save best model state\n",
    "    model_save_path = f\"models/Regression/{dataset_name}/Hybrid/{model_name}/best_model.pth\"\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(best_model_state, model_save_path)\n",
    "    print(f\"Best model saved to {model_save_path}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbdd4c61-b387-425f-88f8-f5f837a32485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_rmse: 6.14e+02 | val_rmse: 7.21e+02 | : 100%|███████████████| 20/20 [02:13<00:00,  6.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 134.00 seconds\n",
      "Best model found at epoch 13/20\n",
      "Best Train Loss: 407559.8262, Best Val Loss: 475892.2276\n",
      "Best Train MSE: 402848.2812, Best Val MSE: 502718.6562\n",
      "Best Train RMSE: 634.7033, Best Val RMSE: 709.0266\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model2/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "metrics = compile_and_fit_2(model2, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model2\", device='cuda',\n",
    "                            batch_size=32, epochs=20, min_lr=1e-4, max_lr=4e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b6ebc61-0b44-4f8b-b488-060e12de9f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_rmse: 3.38e+02 | val_rmse: 6.71e+02 | : 100%|███████████████| 40/40 [04:29<00:00,  6.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 269.98 seconds\n",
      "Best model found at epoch 27/40\n",
      "Best Train Loss: 144957.2822, Best Val Loss: 382101.1964\n",
      "Best Train MSE: 137852.3281, Best Val MSE: 404419.0938\n",
      "Best Train RMSE: 371.2847, Best Val RMSE: 635.9395\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model2/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "metrics = compile_and_fit_2(model2, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model2\", device='cuda',\n",
    "                            batch_size=32, epochs=40, min_lr=1e-4, max_lr=4e-1, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e930097-f4f4-4eac-ba70-8027cc07790e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 156 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2.final_kan.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba491194-9d5c-4a14-baae-22decfe5490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit_hybrid(model, train_loader, val_loader, test_loader,\n",
    "                           dataset_name, model_name,\n",
    "                           batch_size=32, epochs=10,\n",
    "                           min_lr=1e-4, max_lr=4e-3,\n",
    "                           device='cuda', weight_decay=1e-2,\n",
    "                           update_grid=True, grid_update_num=10,\n",
    "                           start_grid_update_step=0, stop_grid_update_step=50,\n",
    "                           lamb=0.0, lamb_l1=1.0, lamb_entropy=2.0, \n",
    "                           lamb_coef=0.0, lamb_coefdiff=0.0,\n",
    "                           singularity_avoiding=False, y_th=1000.,\n",
    "                           save_fig=False, save_fig_freq=1, img_folder='./video'):\n",
    "    \"\"\"\n",
    "    Trains a hybrid model (KAN branch + CNN branch) with features inspired by KAN.fit.\n",
    "    \n",
    "    This routine updates the grid for the m_kan branch periodically and adds regularization\n",
    "    penalties if enabled. It uses OneCycleLR for learning rate scheduling and saves the best model.\n",
    "    \n",
    "    Note: The grid update for the final KAN branch is disabled by default to avoid issues in curve2coef.\n",
    "    \n",
    "    Returns:\n",
    "        metrics: dictionary containing losses, MSE, RMSE, R2 and timing information.\n",
    "    \"\"\"\n",
    "    import copy, os, time, gc\n",
    "    from tqdm import tqdm\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=(max_lr/min_lr),\n",
    "                            total_steps=total_steps, pct_start=0.3, final_div_factor=1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'learning_rate': [], 'epoch_time': []}\n",
    "    \n",
    "    # Determine grid update frequency (in steps)\n",
    "    grid_update_freq = total_steps // grid_update_num if update_grid else None\n",
    "    global_step = 0\n",
    "    overall_start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(range(epochs), desc='Epoch', ncols=100)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss_epoch = 0.0\n",
    "        \n",
    "        for batch_idx, (num_data, img_data, targets) in enumerate(train_loader):\n",
    "            num_data = num_data.to(device, non_blocking=True)\n",
    "            img_data = img_data.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform grid update if enabled and within scheduled steps\n",
    "            if update_grid and grid_update_freq is not None:\n",
    "                if (global_step % grid_update_freq == 0 and \n",
    "                    global_step >= start_grid_update_step and \n",
    "                    global_step < stop_grid_update_step):\n",
    "                    # Update grid for the m_kan branch only\n",
    "                    if hasattr(model.m_kan, 'update_grid'):\n",
    "                        model.m_kan.update_grid(num_data)\n",
    "                    \n",
    "                    # Optionally, you could update final_kan's grid.\n",
    "                    # This is commented out because it may trigger the curve2coef error.\n",
    "                    try:\n",
    "                        with torch.no_grad():\n",
    "                            kan_out = model.m_kan(num_data)\n",
    "                            cnn_out = model.cnn_branch(img_data)\n",
    "                            concat_in = torch.cat((kan_out, cnn_out), dim=1)\n",
    "                        if hasattr(model.final_kan, 'update_grid'):\n",
    "                            model.final_kan.update_grid(concat_in)\n",
    "                    except Exception as e:\n",
    "                        print(\"Warning: final_kan grid update failed:\", e)\n",
    "            \n",
    "            # Forward pass through the hybrid model\n",
    "            outputs = model(num_data, img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            # Optionally add regularization from the KAN branches (if enabled)\n",
    "            reg_loss = 0.0\n",
    "            if hasattr(model.m_kan, 'save_act') and model.m_kan.save_act:\n",
    "                reg_loss += model.m_kan.get_reg('edge_forward_spline_n', lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            if hasattr(model.final_kan, 'save_act') and model.final_kan.save_act:\n",
    "                reg_loss += model.final_kan.get_reg('edge_forward_spline_n', lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            \n",
    "            total_loss = loss + lamb * reg_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss_epoch += total_loss.item()\n",
    "            global_step += 1\n",
    "        \n",
    "        train_loss_epoch /= len(train_loader)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0.0\n",
    "        with torch.no_grad():\n",
    "            for num_data, img_data, targets in val_loader:\n",
    "                num_data = num_data.to(device, non_blocking=True)\n",
    "                img_data = img_data.to(device, non_blocking=True)\n",
    "                targets = targets.to(device, non_blocking=True)\n",
    "                outputs = model(num_data, img_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss_epoch += loss.item()\n",
    "        val_loss_epoch /= len(val_loader)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        history['train_loss'].append(train_loss_epoch)\n",
    "        history['val_loss'].append(val_loss_epoch)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        \n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{epochs} | train_loss: {train_loss_epoch:.2e} | val_loss: {val_loss_epoch:.2e}\")\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if val_loss_epoch < best_val_loss:\n",
    "            best_val_loss = val_loss_epoch\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "        \n",
    "        # Optionally, save model plots if available\n",
    "        if save_fig and (epoch + 1) % save_fig_freq == 0:\n",
    "            if hasattr(model, 'plot'):\n",
    "                model.plot(folder=img_folder, title=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "    total_time = time.time() - overall_start_time\n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "    print(f\"Best model found at epoch {best_epoch} with validation loss {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Calculate metrics on train, validation, and test sets\n",
    "    train_metrics = calculate_metrics(model, train_loader, device)\n",
    "    val_metrics = calculate_metrics(model, val_loader, device)\n",
    "    test_metrics = calculate_metrics(model, test_loader, device)\n",
    "    \n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'train_mse': train_metrics['mse'],\n",
    "        'train_mae': train_metrics['mae'],\n",
    "        'train_rmse': train_metrics['rmse'],\n",
    "        'train_r2': train_metrics['r2'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_mse': val_metrics['mse'],\n",
    "        'val_mae': val_metrics['mae'],\n",
    "        'val_rmse': val_metrics['rmse'],\n",
    "        'val_r2': val_metrics['r2'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_mse': test_metrics['mse'],\n",
    "        'test_mae': test_metrics['mae'],\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'test_r2': test_metrics['r2'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "    \n",
    "    # Save best model state\n",
    "    model_save_path = f\"models/Regression/{dataset_name}/Hybrid/{model_name}/best_model.pth\"\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(best_model_state, model_save_path)\n",
    "    print(f\"Best model saved to {model_save_path}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6688a338-b814-4b0e-a9ad-54dc68be8028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|                                                                 | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1/20 | train_loss: 4.85e+05 | val_loss: 5.41e+05:   0%|                | 0/20 [00:08<?, ?it/s]\u001b[A\n",
      "Epoch 2/20 | train_loss: 4.73e+05 | val_loss: 5.10e+05:   0%|                | 0/20 [00:15<?, ?it/s]\u001b[A\n",
      "Epoch 3/20 | train_loss: 4.48e+05 | val_loss: 4.77e+05:   0%|                | 0/20 [00:22<?, ?it/s]\u001b[A\n",
      "Epoch 4/20 | train_loss: 4.28e+05 | val_loss: 4.84e+05:   0%|                | 0/20 [00:29<?, ?it/s]\u001b[A\n",
      "Epoch 5/20 | train_loss: 4.44e+05 | val_loss: 5.84e+05:   0%|                | 0/20 [00:36<?, ?it/s]\u001b[A\n",
      "Epoch 6/20 | train_loss: 4.03e+05 | val_loss: 5.42e+05:   0%|                | 0/20 [00:43<?, ?it/s]\u001b[A\n",
      "Epoch 7/20 | train_loss: 4.57e+05 | val_loss: 5.02e+05:   0%|                | 0/20 [00:50<?, ?it/s]\u001b[A\n",
      "Epoch 8/20 | train_loss: 4.45e+05 | val_loss: 5.35e+05:   0%|                | 0/20 [00:57<?, ?it/s]\u001b[A\n",
      "Epoch 9/20 | train_loss: 4.47e+05 | val_loss: 5.06e+05:   0%|                | 0/20 [01:03<?, ?it/s]\u001b[A\n",
      "Epoch 10/20 | train_loss: 4.49e+05 | val_loss: 5.06e+05:   0%|               | 0/20 [01:10<?, ?it/s]\u001b[A\n",
      "Epoch 11/20 | train_loss: 4.32e+05 | val_loss: 4.61e+05:   0%|               | 0/20 [01:18<?, ?it/s]\u001b[A\n",
      "Epoch 12/20 | train_loss: 4.31e+05 | val_loss: 5.74e+05:   0%|               | 0/20 [01:25<?, ?it/s]\u001b[A\n",
      "Epoch 13/20 | train_loss: 4.33e+05 | val_loss: 4.19e+05:   0%|               | 0/20 [01:32<?, ?it/s]\u001b[A\n",
      "Epoch 14/20 | train_loss: 4.10e+05 | val_loss: 4.51e+05:   0%|               | 0/20 [01:39<?, ?it/s]\u001b[A\n",
      "Epoch 15/20 | train_loss: 3.90e+05 | val_loss: 4.38e+05:   0%|               | 0/20 [01:46<?, ?it/s]\u001b[A\n",
      "Epoch 16/20 | train_loss: 3.83e+05 | val_loss: 4.20e+05:   0%|               | 0/20 [01:53<?, ?it/s]\u001b[A\n",
      "Epoch 17/20 | train_loss: 3.70e+05 | val_loss: 4.28e+05:   0%|               | 0/20 [02:00<?, ?it/s]\u001b[A\n",
      "Epoch 18/20 | train_loss: 3.59e+05 | val_loss: 4.25e+05:   0%|               | 0/20 [02:07<?, ?it/s]\u001b[A\n",
      "Epoch 19/20 | train_loss: 3.62e+05 | val_loss: 4.32e+05:   0%|               | 0/20 [02:14<?, ?it/s]\u001b[A\n",
      "Epoch 20/20 | train_loss: 3.65e+05 | val_loss: 4.32e+05:   0%|               | 0/20 [02:22<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 142.04 seconds\n",
      "Best model found at epoch 13 with validation loss 419148.0476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | train_loss: 3.65e+05 | val_loss: 4.32e+05:   0%|               | 0/20 [02:24<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to models/Regression/1000cameras/Hybrid/1000cameras_Model1/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = compile_and_fit_hybrid(\n",
    "    model1, train_loader, val_loader, test_loader,\n",
    "    dataset_name, f\"{dataset_name}_Model1\",\n",
    "    batch_size=32, epochs=20,\n",
    "    min_lr=1e-4, max_lr=4e-1,\n",
    "    device='cuda', weight_decay=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733988b-13b8-411c-a204-7e196ad08be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|                                                                 | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstsq failed\n",
      "Warning: final_kan grid update failed: cannot access local variable 'coef' where it is not associated with a value\n"
     ]
    }
   ],
   "source": [
    "metrics = compile_and_fit_hybrid(\n",
    "    model2, train_loader, val_loader, test_loader,\n",
    "    dataset_name, f\"{dataset_name}_Model1\",\n",
    "    batch_size=32, epochs=10,\n",
    "    min_lr=1e-4, max_lr=4e-1,\n",
    "    device='cuda', weight_decay=1e-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
