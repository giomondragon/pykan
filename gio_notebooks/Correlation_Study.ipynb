{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "16d3b58e-a4de-4779-bfaa-20eaf4af4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab1aa4e-ce9e-48df-b990-b70a1e85bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52ed14ba-9683-4017-ba59-be148662792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean(npy_filename, y_filename, x_col, target_col):\n",
    "    \"\"\"\n",
    "    Load the feature array (npy_filename) and label array (y_filename),\n",
    "    drop rows in the feature array that contain any NaNs, and apply\n",
    "    the same mask to the label array.\n",
    "    \"\"\"\n",
    "    # Load numpy arrays\n",
    "    X = np.load(os.path.join(folder, npy_filename))\n",
    "    y = np.load(os.path.join(folder, y_filename), allow_pickle=True)\n",
    "    \n",
    "    # Ensure the number of rows matches between X and y\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"The number of rows in {} and {} do not match.\".format(npy_filename, y_filename))\n",
    "    \n",
    "    # Create a boolean mask for rows that do NOT have any NaN values in X\n",
    "    valid_rows = ~np.isnan(X).any(axis=1)\n",
    "    #print(valid_rows)\n",
    "    # Filter both arrays using the valid_rows mask\n",
    "    X_clean = X[valid_rows]\n",
    "    y_clean = y[valid_rows]\n",
    "    \n",
    "    # Convert arrays to DataFrames\n",
    "    df_X = pd.DataFrame(X_clean)\n",
    "    df_y = pd.DataFrame(y_clean)\n",
    "    df_X.columns = x_col\n",
    "    df_y.columns = target_col\n",
    "\n",
    "    df_y[target_col] = df_y[target_col].astype(int)\n",
    "    return df_X, df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8a4efbee-e806-4863-9de6-8d182de89f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(df_num, dataset_name):\n",
    "    df_num = df_num.select_dtypes(include=[np.number])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_num)\n",
    "\n",
    "    # Correlation Analysis\n",
    "    corr_matrix = pd.DataFrame(df_scaled, columns=df_num.columns).corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    mean_corr = upper_tri.stack().mean()\n",
    "    std_corr = upper_tri.stack().std()\n",
    "\n",
    "    # Distance + Feature Clustering\n",
    "    dist_matrix = pairwise_distances(df_scaled.T, metric='euclidean')\n",
    "    dist_condensed = pdist(df_scaled.T)\n",
    "    linkage_matrix = linkage(dist_condensed, method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=2, criterion='maxclust')\n",
    "    silhouette = silhouette_score(dist_matrix, clusters, metric='precomputed') if len(set(clusters)) > 1 else 0\n",
    "\n",
    "    # Feature Continuity Score: average adjacent feature correlation (simulate 1D layout locality)\n",
    "    feature_continuity = np.mean([np.corrcoef(df_scaled[:, i], df_scaled[:, i+1])[0, 1]\n",
    "                                  for i in range(df_scaled.shape[1] - 1)])\n",
    "\n",
    "    # Feature interaction proxy: variance of pairwise feature product means\n",
    "    interaction_values = []\n",
    "    for i in range(df_scaled.shape[1]):\n",
    "        for j in range(i + 1, df_scaled.shape[1]):\n",
    "            interaction_values.append(np.mean(df_scaled[:, i] * df_scaled[:, j]))\n",
    "    interaction_var = np.var(interaction_values)\n",
    "\n",
    "    # Normalized metrics\n",
    "    norm_corr = min(mean_corr / 1.0, 1.0)\n",
    "    norm_silhouette = max(silhouette, 0)\n",
    "    norm_dim = min(df_scaled.shape[1] / 100, 1.0)\n",
    "    norm_continuity = max(min(feature_continuity, 1), 0)\n",
    "    norm_interaction = min(interaction_var / 1.0, 1.0)\n",
    "\n",
    "    # Weighted Suitability Score\n",
    "    suitability_score = (\n",
    "        0.35 * norm_corr +\n",
    "        0.26 * norm_silhouette +\n",
    "        0.13 * norm_dim + \n",
    "        0.13 * norm_continuity +\n",
    "        0.13 * norm_interaction\n",
    "    )\n",
    "\n",
    "    if suitability_score >= 0.66:\n",
    "        category = 'High Correlation Suitability'\n",
    "    elif suitability_score >= 0.33:\n",
    "        category = 'Medium Correlation Suitability'\n",
    "    else:\n",
    "        category = 'Low Correlation Suitability'\n",
    "\n",
    "    result = {\n",
    "        'dataset': dataset_name,\n",
    "        'n_samples': df_scaled.shape[0],\n",
    "        'n_num_features': df_scaled.shape[1],\n",
    "        'mean_corr': float(mean_corr),\n",
    "        'std_corr': float(std_corr),\n",
    "        'silhouette': float(silhouette),\n",
    "        'feature_continuity': float(feature_continuity),\n",
    "        'interaction_var': float(interaction_var),\n",
    "        'suitability_score': float(suitability_score),\n",
    "        'category': category\n",
    "    }\n",
    "\n",
    "    print(f\"[DONE] {dataset_name}: Score={suitability_score:.3f} → {category}\")\n",
    "    # Compute average absolute correlation per feature (excluding self-correlation)\n",
    "    feature_scores = corr_matrix.copy()\n",
    "    np.fill_diagonal(feature_scores.values, np.nan)  # ignore self-correlation\n",
    "    feature_avg_corr = feature_scores.mean(axis=1).sort_values(ascending=False)\n",
    "    # Show as a table\n",
    "    feature_table = pd.DataFrame({\n",
    "        'Feature': feature_avg_corr.index,\n",
    "        'Avg Absolute Correlation': feature_avg_corr.values\n",
    "    })\n",
    "    \n",
    "    print(\"\\n=== Average Absolute Correlation Per Feature ===\")\n",
    "    print(feature_table.to_string(index=False))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bce7b7f-dca3-4dd6-a296-cae95c0972a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col=[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "60abda67-e82f-4806-b4e1-4725c6957890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] data/treasury: Score=0.666 → High Correlation Suitability\n",
      "\n",
      "=== Average Absolute Correlation Per Feature ===\n",
      "                Feature  Avg Absolute Correlation\n",
      " 3M-Rate-AuctionAverage                  0.881946\n",
      "      checkableDeposits                  0.872843\n",
      "             bankCredit                  0.871836\n",
      "       5Y-CMaturityRate                  0.866276\n",
      "      30Y-CMortgageRate                  0.844544\n",
      "        tradeCurrencies                  0.838322\n",
      "            loansLeases                  0.834138\n",
      "3M-Rate-SecondaryMarket                  0.826873\n",
      "       3Y-CMaturityRate                  0.825911\n",
      "               currency                  0.825009\n",
      "             moneyStock                  0.816039\n",
      "        savingsDeposits                  0.815735\n",
      "         demandDeposits                  0.801799\n",
      "           federalFunds                  0.801674\n",
      "       1Y-CMaturityRate                  0.562426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': 'data/treasury',\n",
       " 'n_samples': 671,\n",
       " 'n_num_features': 15,\n",
       " 'mean_corr': 0.8190246831610097,\n",
       " 'std_corr': 0.1340964965385351,\n",
       " 'silhouette': 0.7702299002045945,\n",
       " 'feature_continuity': 0.5387911062077669,\n",
       " 'interaction_var': 0.688590223117676,\n",
       " 'suitability_score': 0.6659779859718555,\n",
       " 'category': 'High Correlation Suitability'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder=\"data/treasury\"\n",
    "x_col=[\"1Y-CMaturityRate\", \"30Y-CMortgageRate\", \"3M-Rate-AuctionAverage\", \"3M-Rate-SecondaryMarket\", \"3Y-CMaturityRate\", \n",
    "       \"5Y-CMaturityRate\", \"bankCredit\", \"currency\", \"demandDeposits\", \"federalFunds\", \"moneyStock\", \"checkableDeposits\", \n",
    "       \"loansLeases\", \"savingsDeposits\", \"tradeCurrencies\"]\n",
    "X_train, _ = load_and_clean('N_train.npy', 'y_train.npy',x_col, target_col)\n",
    "analyze_dataset(X_train, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "18417b29-a294-4b25-b401-9bd69ac70076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] data/puma8NH: Score=0.015 → Low Correlation Suitability\n",
      "\n",
      "=== Average Absolute Correlation Per Feature ===\n",
      "Feature  Avg Absolute Correlation\n",
      "   tau2                  0.011663\n",
      " theta2                  0.010963\n",
      " theta1                  0.009622\n",
      " theta3                  0.007331\n",
      "thetad2                  0.007282\n",
      "   tau1                  0.006976\n",
      "thetad3                  0.006474\n",
      "thetad1                  0.005836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': 'data/puma8NH',\n",
       " 'n_samples': 5242,\n",
       " 'n_num_features': 8,\n",
       " 'mean_corr': 0.008268348352486755,\n",
       " 'std_corr': 0.00650531152108807,\n",
       " 'silhouette': 0.004769042974062304,\n",
       " 'feature_continuity': 0.0022739606162350046,\n",
       " 'interaction_var': 0.00010764769466484429,\n",
       " 'suitability_score': 0.014843482177043543,\n",
       " 'category': 'Low Correlation Suitability'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder=\"data/puma8NH\"\n",
    "x_col=[\"theta1\", \"theta2\", \"theta3\", \"thetad1\", \"thetad2\", \"thetad3\", \"tau1\",\"tau2\"]\n",
    "X_train, _ = load_and_clean('N_train.npy', 'y_train.npy',x_col, target_col)\n",
    "analyze_dataset(X_train, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0c98133c-d3cd-4377-a8fa-0015a14e1c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] data/FOREX_cadjpy-day-High: Score=0.645 → Medium Correlation Suitability\n",
      "\n",
      "=== Average Absolute Correlation Per Feature ===\n",
      "   Feature  Avg Absolute Correlation\n",
      "   Ask_Low                  0.845501\n",
      "   Bid_Low                  0.845448\n",
      " Bid_Close                  0.843024\n",
      " Ask_Close                  0.843024\n",
      "  Ask_Open                  0.841695\n",
      "  Bid_Open                  0.841624\n",
      "  Ask_High                  0.840112\n",
      "  Bid_High                  0.840089\n",
      "Bid_Volume                  0.378499\n",
      "Ask_Volume                  0.368663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': 'data/FOREX_cadjpy-day-High',\n",
       " 'n_samples': 1173,\n",
       " 'n_num_features': 10,\n",
       " 'mean_corr': 0.7487677680675592,\n",
       " 'std_corr': 0.3388843248518122,\n",
       " 'silhouette': 0.9482354646130104,\n",
       " 'feature_continuity': 0.5654130508093214,\n",
       " 'interaction_var': 0.38451655466092927,\n",
       " 'suitability_score': 0.6451007883341608,\n",
       " 'category': 'Medium Correlation Suitability'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder=\"data/FOREX_cadjpy-day-High\"\n",
    "x_col=[\"Bid_Open\", \"Bid_High\", \"Bid_Low\", \"Bid_Close\", \"Bid_Volume\", \"Ask_Open\", \"Ask_High\", \"Ask_Low\", \"Ask_Close\",\"Ask_Volume\"]\n",
    "X_train, _ = load_and_clean('N_train.npy', 'y_train.npy',x_col, target_col)\n",
    "analyze_dataset(X_train, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "18962334-ab15-431d-8305-c908e75ad563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] data/wall-robot-navigation: Score=0.182 → Low Correlation Suitability\n",
      "\n",
      "=== Average Absolute Correlation Per Feature ===\n",
      "Feature  Avg Absolute Correlation\n",
      "     V9                  0.216733\n",
      "    V10                  0.201590\n",
      "    V22                  0.201490\n",
      "     V1                  0.200496\n",
      "     V7                  0.200118\n",
      "     V8                  0.194734\n",
      "    V21                  0.193048\n",
      "    V23                  0.189889\n",
      "    V24                  0.184586\n",
      "     V4                  0.178936\n",
      "     V6                  0.178921\n",
      "     V5                  0.177320\n",
      "    V18                  0.175391\n",
      "    V11                  0.173374\n",
      "    V14                  0.164253\n",
      "    V19                  0.161838\n",
      "    V12                  0.159364\n",
      "    V17                  0.155663\n",
      "     V3                  0.144565\n",
      "    V20                  0.142113\n",
      "    V13                  0.140473\n",
      "    V15                  0.134190\n",
      "     V2                  0.105729\n",
      "    V16                  0.098139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': 'data/wall-robot-navigation',\n",
       " 'n_samples': 3491,\n",
       " 'n_num_features': 24,\n",
       " 'mean_corr': 0.169706349687189,\n",
       " 'std_corr': 0.11912759022047617,\n",
       " 'silhouette': 0.13050503493449714,\n",
       " 'feature_continuity': 0.4000057581485569,\n",
       " 'interaction_var': 0.04291495250784353,\n",
       " 'suitability_score': 0.18210822385881748,\n",
       " 'category': 'Low Correlation Suitability'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder=\"data/wall-robot-navigation\"\n",
    "x_col=[\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\",\"V9\",\"V10\",\"V11\", \"V12\", \"V13\", \n",
    "       \"V14\", \"V15\", \"V16\", \"V17\", \"V18\",\"V19\",\"V20\",\"V21\", \"V22\", \"V23\", \"V24\"]\n",
    "X_train, _ = load_and_clean('N_train.npy', 'y_train.npy',x_col, target_col)\n",
    "analyze_dataset(X_train, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a65656-6acc-4f9c-a1c1-eeddd43aa234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a06607-a118-4b37-9db8-633395dc405b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
