{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dea99dd-13b0-4b9f-8497-4b83f72afc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the root directory\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Add the root directory to the Python path\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5dcb076-7843-40fe-a0d5-850312d9f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from TINTOlib.tinto import TINTO\n",
    "from vit_pytorch.vit import ViT\n",
    "from kan import *\n",
    "\n",
    "\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import traceback\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a604e69-efd9-4720-8de5-0da057312df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4af556-2b79-408e-b782-9f45c1860ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 381\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317dfa67-934c-48f1-a237-69f908076e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"data/1000-Cameras-Dataset\"\n",
    "x_col=[\"Release_date\", \"Max_resolution\", \"Low_resolution\", \"Effective_pixels\", \"Zoom\", \"Normal_focus_range\", \"Macro_focus_range\", \"Storage_included\",\"Weight\",\"Dimensions\"]\n",
    "target_col=[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "279ac1c3-cd38-494f-8fcd-2b02e482bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean(npy_filename, y_filename, x_col, target_col):\n",
    "    \"\"\"\n",
    "    Load the feature array (npy_filename) and label array (y_filename),\n",
    "    drop rows in the feature array that contain any NaNs, and apply\n",
    "    the same mask to the label array.\n",
    "    \"\"\"\n",
    "    # Load numpy arrays\n",
    "    X = np.load(os.path.join(folder, npy_filename))\n",
    "    y = np.load(os.path.join(folder, y_filename))\n",
    "    \n",
    "    # Ensure the number of rows matches between X and y\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"The number of rows in {} and {} do not match.\".format(npy_filename, y_filename))\n",
    "    \n",
    "    # Create a boolean mask for rows that do NOT have any NaN values in X\n",
    "    valid_rows = ~np.isnan(X).any(axis=1)\n",
    "\n",
    "    # Filter both arrays using the valid_rows mask\n",
    "    X_clean = X[valid_rows]\n",
    "    y_clean = y[valid_rows]\n",
    "    \n",
    "    # Convert arrays to DataFrames\n",
    "    df_X = pd.DataFrame(X_clean)\n",
    "    df_y = pd.DataFrame(y_clean)\n",
    "    df_X.columns = x_col\n",
    "    df_y.columns = target_col\n",
    "    return df_X, df_y\n",
    "\n",
    "X_train, y_train = load_and_clean('N_train.npy', 'y_train.npy',x_col, target_col)\n",
    "X_test, y_test   = load_and_clean('N_test.npy',  'y_test.npy', x_col, target_col)\n",
    "X_val, y_val     = load_and_clean('N_val.npy',   'y_val.npy', x_col, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f90af460-8f7b-4cc9-aa1c-0799bab6ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train, y_train], axis=1)\n",
    "X_test = pd.concat([X_test, y_test], axis=1)\n",
    "X_val = pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0cbb5b-9cfc-4e39-bbe9-3e526ada260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(X_train, X_test, X_val, images_folder, image_model, problem_type, batch_size=32, device='cuda'):\n",
    "    \"\"\"Loads, processes, and converts a dataset into PyTorch DataLoaders for a ViT + KAN model.\"\"\"\n",
    "    \n",
    "    # Reset indices\n",
    "    data_splits = {\n",
    "        \"train\": X_train.reset_index(drop=True), \n",
    "        \"val\": X_val.reset_index(drop=True), \n",
    "        \"test\": X_test.reset_index(drop=True)\n",
    "    }\n",
    "    \n",
    "    # Process images and numerical features\n",
    "    num_features, img_paths, targets = {}, {}, {}\n",
    "\n",
    "    for split, X_split in data_splits.items():\n",
    "        split_folder = f\"{images_folder}/{split}\"\n",
    "        os.makedirs(split_folder, exist_ok=True)\n",
    "\n",
    "        # Ensure images and CSV files are generated\n",
    "        image_model.fit_transform(X_split, split_folder) if split == \"train\" else image_model.transform(X_split, split_folder)\n",
    "\n",
    "        # Validate CSV file existence\n",
    "        csv_path = os.path.join(split_folder, f\"{problem_type}.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"Error: Missing CSV at {csv_path}. Available files: {os.listdir(split_folder)}\")\n",
    "            raise FileNotFoundError(f\"Expected file {csv_path} not found. Ensure image_model generates it.\")\n",
    "\n",
    "        print(f\"Loading {csv_path}...\")\n",
    "        img_df = pd.read_csv(csv_path)\n",
    "        img_df[\"images\"] = split_folder + \"/\" + img_df[\"images\"]\n",
    "\n",
    "        # Combine numerical data and images\n",
    "        combined_df = pd.concat([img_df, X_split], axis=1)\n",
    "        num_features[split] = combined_df.drop(columns=[\"values\", \"images\", X_train.columns[-1]])\n",
    "        img_paths[split] = img_df[\"images\"]\n",
    "        targets[split] = combined_df[\"values\"]\n",
    "\n",
    "    # Standardize numerical data\n",
    "    scaler = StandardScaler()\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        num_features[split] = pd.DataFrame(scaler.fit_transform(num_features[split]) if split == \"train\" \n",
    "                                           else scaler.transform(num_features[split]), \n",
    "                                           columns=num_features[split].columns)\n",
    "\n",
    "    # Convert images to tensors\n",
    "    img_tensors = {split: torch.tensor(np.array([cv2.imread(img) for img in img_paths[split]]), \n",
    "                                       dtype=torch.float32, device=device).permute(0, 3, 1, 2) / 255.0\n",
    "                   for split in [\"train\", \"val\", \"test\"]}\n",
    "    \n",
    "    # Get dataset properties\n",
    "    attributes = num_features[\"train\"].shape[1]\n",
    "    channels, height, width = img_tensors[\"train\"].shape[1:]\n",
    "    imgs_shape = (channels, height, width)\n",
    "\n",
    "    print(\"Images shape:\", imgs_shape)\n",
    "    print(\"Attributes:\", attributes)\n",
    "\n",
    "    # Convert numerical data and targets to tensors\n",
    "    num_tensors = {split: torch.tensor(num_features[split].values, dtype=torch.float32, device=device)\n",
    "                   for split in [\"train\", \"val\", \"test\"]}\n",
    "    \n",
    "    target_tensors = {split: torch.tensor(targets[split].values, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "                      for split in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "    # Create DataLoaders\n",
    "    datasets = {split: TensorDataset(num_tensors[split], img_tensors[split], target_tensors[split]) \n",
    "                for split in [\"train\", \"val\", \"test\"]}\n",
    "    \n",
    "    data_loaders = {split: DataLoader(datasets[split], batch_size=batch_size, shuffle=(split == \"train\")) \n",
    "                    for split in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "    return data_loaders[\"train\"], data_loaders[\"val\"], data_loaders[\"test\"], attributes, imgs_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72c5d850-d142-4680-b4ce-7f3d4173cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TINTOlib.tinto import TINTO\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem=problem_type, blur=True, random_seed=SEED)\n",
    "images_folder = \"./HyKANImages/1000Cameras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81e4d4a5-6d27-486a-adb1-040ea011edf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./HyKANImages/1000Cameras/train\\regression.csv...\n",
      "Loading ./HyKANImages/1000Cameras/val\\regression.csv...\n",
      "Loading ./HyKANImages/1000Cameras/test\\regression.csv...\n",
      "Images shape: (3, 20, 20)\n",
      "Attributes: 10\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape = load_and_preprocess_data(\n",
    "    X_train, X_test, X_val,\n",
    "    images_folder=images_folder,\n",
    "    image_model=image_model,\n",
    "    problem_type=problem_type,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4417027d-175b-499f-8ed5-daea843d3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, patch_size, device):\n",
    "        super(Model1, self).__init__()\n",
    "        self.device = device\n",
    "        # ViT branch\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape,\n",
    "            patch_size = patch_size,\n",
    "            dim = 32,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "            mlp_dim = 64,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        ).to(device)\n",
    "\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, 10],\n",
    "            grid=3,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        self.final_kan = KAN(\n",
    "            width=[32+10, 1],\n",
    "            grid=4,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, mlp_input, vit_input):\n",
    "        # Ensure inputs are moved to the correct device\n",
    "        kan_input = mlp_input.to(self.device)\n",
    "        vit_input = vit_input.to(self.device)\n",
    "        \n",
    "        vit_output = self.vit(vit_input)  # Process image input\n",
    "        kan_output = self.m_kan(kan_input)  # Process numerical input\n",
    "        \n",
    "        concat_output = torch.cat((kan_output, vit_output), dim=1)\n",
    "        return self.final_kan(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b756eb7-ce9b-465e-a04a-33db3e59a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_create_model(model_class, patch_size, attributes, imgs_shape, device):\n",
    "    try:\n",
    "        model = model_class(attributes, imgs_shape[1:], patch_size, device)\n",
    "        \n",
    "        # Test the model with a sample input\n",
    "        num_input = torch.randn(4, attributes)\n",
    "        img_input = torch.randn(4, *imgs_shape)\n",
    "        output = model(num_input, img_input)\n",
    "        \n",
    "        print(f\"Successfully created and tested {model_class.__name__}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or testing {model_class.__name__}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae16e667-1905-4eca-bcc6-7934be63846d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model1\n"
     ]
    }
   ],
   "source": [
    "patch_size = 2\n",
    "model1 = try_create_model(Model1, patch_size, attributes, imgs_shape, device)  # Attempt to create Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b768c0de-22f7-481a-8efd-63d0ca7443ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x19bd5e26300>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b91efae-7e62-4b64-bc94-a2606f4cee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=32, epochs=10, min_lr=1e-3, max_lr=1, device='cuda', weight_decay=1e-2):\n",
    "    model = model\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=max_lr/min_lr, total_steps=total_steps, pct_start=0.3, final_div_factor=1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mse': [], 'val_mse': [], 'train_rmse': [], 'val_rmse': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "        for num_data, img_data, targets in train_loader:\n",
    "            num_data, img_data, targets = num_data.to(device, non_blocking=True), img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(num_data, img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for num_data, img_data, targets in val_loader:\n",
    "                num_data, img_data, targets = num_data.to(device, non_blocking=True), img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                outputs = model(num_data, img_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "        \n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "        train_mse = mean_squared_error(train_targets, train_predictions)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        val_mse = mean_squared_error(val_targets, val_predictions)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        train_r2 = r2_score(train_targets, train_predictions)\n",
    "        val_r2 = r2_score(val_targets, val_predictions)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "        history['val_mse'].append(val_mse)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    train_metrics = calculate_metrics(model, train_loader, device)\n",
    "    val_metrics = calculate_metrics(model, val_loader, device)\n",
    "    test_metrics = calculate_metrics(model, test_loader, device)\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'train_mse': train_metrics['mse'],\n",
    "        'train_mae': train_metrics['mae'],\n",
    "        'train_rmse': train_metrics['rmse'],\n",
    "        'train_r2': train_metrics['r2'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_mse': val_metrics['mse'],\n",
    "        'val_mae': val_metrics['mae'],\n",
    "        'val_rmse': val_metrics['rmse'],\n",
    "        'val_r2': val_metrics['r2'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_mse': test_metrics['mse'],\n",
    "        'test_mae': test_metrics['mae'],\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'test_r2': test_metrics['r2'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "    print(f\"Best model found at epoch {best_epoch}/{epochs}\")\n",
    "    print(f\"Best Train Loss: {history['train_loss'][best_epoch-1]:.4f}, Best Val Loss: {history['val_loss'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train MSE: {history['train_mse'][best_epoch-1]:.4f}, Best Val MSE: {history['val_mse'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train RMSE: {history['train_rmse'][best_epoch-1]:.4f}, Best Val RMSE: {history['val_rmse'][best_epoch-1]:.4f}\")\n",
    "\n",
    "    # Save metrics to a file\n",
    "    os.makedirs(f'logs/Regression/{dataset_name}/ViT+MLP/{model_name}', exist_ok=True)\n",
    "    with open(f'logs/Regression/{dataset_name}/ViT+MLP/{model_name}/metrics.txt', 'w') as f:\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "            \n",
    "    # Save best model\n",
    "    model_save_path = f\"models/Regression/{dataset_name}/ViT+MLP/{model_name}/best_model.pth\"\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(best_model, model_save_path)\n",
    "    print(f\"Best model saved to {model_save_path}\")\n",
    "            \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def calculate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for num_data, img_data, targets in data_loader:\n",
    "            num_data, img_data, targets = num_data.to(device, non_blocking=True), img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(num_data, img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "    mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_predictions)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fc290ea-d0dd-4167-8cd4-a4f5ba5f4de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 298.79 seconds\n",
      "Best model found at epoch 120/120\n",
      "Best Train Loss: 352820.3616, Best Val Loss: 389188.4089\n",
      "Best Train MSE: 355401.2812, Best Val MSE: 446055.0312\n",
      "Best Train RMSE: 596.1554, Best Val RMSE: 667.8735\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model1/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "dataset_name = '1000cameras'\n",
    "name = f\"TINTO\"\n",
    "metrics = compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model1\", batch_size=32, epochs=120, min_lr=1e-4, max_lr=4e-3 , device='cuda', weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c02afe2a-6528-4518-b265-42d0ce187337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.7928e-03, 1.6311e-01, 1.5233e-01, 2.9513e-02, 1.6435e-01, 1.6471e-01,\n",
       "        1.5986e-01, 6.0910e-02, 4.2652e-03, 1.5801e-01, 2.0108e-04, 1.5379e-04,\n",
       "        1.6519e-03, 2.6485e-04, 1.3705e-03, 2.7821e-03, 6.8541e-04, 2.0551e-03,\n",
       "        4.5704e-04, 1.4279e-03, 3.4299e-03, 3.1923e-03, 2.6532e-03, 2.4447e-03,\n",
       "        2.7929e-03, 2.8580e-03, 2.1313e-03, 6.8872e-05, 3.5130e-03, 9.1692e-04,\n",
       "        1.9453e-04, 3.0952e-03, 2.3563e-04, 3.2348e-04, 1.5216e-03, 2.4780e-03,\n",
       "        1.2614e-04, 2.5982e-04, 3.0383e-04, 1.6315e-03, 1.4432e-04, 2.5951e-03],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.final_kan.feature_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "676b6cba-a9a8-4035-8bf7-27b013d4f08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 44 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1.final_kan.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabae44f-908d-46ae-b357-710c6a765d24",
   "metadata": {},
   "source": [
    "# Usign IGTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3bdde30-d371-47a1-90a7-fb733412b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TINTOlib.refined import REFINED\n",
    "from TINTOlib.igtd import IGTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ebb0f11-d2bc-4f05-a6b0-2c83a9ee449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_size=16\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], random_seed=SEED)\n",
    "name = f\"IGTD_{image_size}x{image_size}\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d088eb9f-802c-4297-b8ed-acbc4f4f9269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./HyNNImages/Regression/1000cameras/images_1000cameras_IGTD_16x16/train\\regression.csv...\n",
      "Loading ./HyNNImages/Regression/1000cameras/images_1000cameras_IGTD_16x16/val\\regression.csv...\n",
      "Loading ./HyNNImages/Regression/1000cameras/images_1000cameras_IGTD_16x16/test\\regression.csv...\n",
      "Images shape: (3, 16, 16)\n",
      "Attributes: 10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAAAnAAAAJwEqCZFPAAAARElEQVR4nO3MQRHAMAgAwbQWsMSgAzdowwZ/ZnBwVdBX8sz9b5eZoaq4OyJCd5OZAMwMEUFV8dcDsDZ6d+YLXOACB4EPL3I4GQpP0hwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 16x16 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAAAnAAAAJwEqCZFPAAAAQ0lEQVR4nO3MsRHAMAjAQCeT0zMJYzAGR8WxBJ0yQSq7tHr9AjAzIgIRYWaoKtydzERV6W7+egDWRu/OfIELXOAg8AGZpj2kVTQBHwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 16x16 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAAAnAAAAJwEqCZFPAAAARElEQVR4nO3MMQHAMAgAwbT2kIECTCADXXhgh/GroFMy5ve/paqYGe6OiDAzZCaqSncTEVQVfz0Aa6N3Z77ABS5wEPgAynk4+n+HESEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 16x16 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape = load_and_preprocess_data(\n",
    "    X_train, X_test, X_val,\n",
    "    images_folder=images_folder,\n",
    "    image_model=image_model,\n",
    "    problem_type=problem_type,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4617d532-9e2d-4e90-99ac-2d355a7cac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 246.73 seconds\n",
      "Best model found at epoch 30/60\n",
      "Best Train Loss: 330912.6596, Best Val Loss: 380447.6169\n",
      "Best Train MSE: 332541.7188, Best Val MSE: 434970.5000\n",
      "Best Train RMSE: 576.6643, Best Val RMSE: 659.5229\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model1/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "dataset_name = '1000cameras'\n",
    "name = f\"IGTD\"\n",
    "metrics = compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model1\", batch_size=32, epochs=60, min_lr=1e-4, max_lr=4e-3 , device='cuda', weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00862739-327b-4f71-99d5-4445ba5ecc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 297.57 seconds\n",
      "Best model found at epoch 99/120\n",
      "Best Train Loss: 252571.6938, Best Val Loss: 367972.1029\n",
      "Best Train MSE: 254601.3125, Best Val MSE: 419688.8750\n",
      "Best Train RMSE: 504.5803, Best Val RMSE: 647.8340\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model1/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "dataset_name = '1000cameras'\n",
    "name = f\"IGTD\"\n",
    "metrics = compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model1\", batch_size=16, epochs=120, min_lr=1e-4, max_lr=4e-3 , device='cuda', weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b5b4e4c-e731-4659-8df5-00857b6ca591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 44 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1.final_kan.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65df4ea2-357c-4abf-a832-c1305c8a6c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0038, 0.1655, 0.1503, 0.0754, 0.1603, 0.1645, 0.1598, 0.1575, 0.0510,\n",
       "        0.1580, 0.0011, 0.0002, 0.0075, 0.0037, 0.0087, 0.0100, 0.0015, 0.0091,\n",
       "        0.0038, 0.0100, 0.0112, 0.0094, 0.0113, 0.0095, 0.0105, 0.0102, 0.0107,\n",
       "        0.0005, 0.0103, 0.0036, 0.0029, 0.0113, 0.0009, 0.0031, 0.0099, 0.0075,\n",
       "        0.0020, 0.0030, 0.0045, 0.0071, 0.0017, 0.0073], device='cuda:0',\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.final_kan.feature_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68b14896-2383-4ef1-bb18-354562a428a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 467.87 seconds\n",
      "Best model found at epoch 135/240\n",
      "Best Train Loss: 178589.5533, Best Val Loss: 359463.0391\n",
      "Best Train MSE: 178265.0938, Best Val MSE: 409286.5938\n",
      "Best Train RMSE: 422.2145, Best Val RMSE: 639.7551\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model1/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "dataset_name = '1000cameras'\n",
    "name = f\"IGTD\"\n",
    "metrics = compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model1\", batch_size=16, epochs=240, min_lr=1e-4, max_lr=4e-3 , device='cuda', weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94a96c15-6dd2-4147-87f6-b2e4362db219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit_2(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=32, epochs=10, min_lr=1e-3, max_lr=1, device='cuda', weight_decay=1e-2):\n",
    "    model = model\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=max_lr/min_lr, total_steps=total_steps, pct_start=0.3, final_div_factor=1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mse': [], 'val_mse': [], 'train_rmse': [], 'val_rmse': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    start_time = time.time()\n",
    "    pbar = tqdm(range(epochs), desc='description', ncols=100)\n",
    "    #for epoch in range(epochs):\n",
    "    for epoch in pbar:\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "        for num_data, img_data, targets in train_loader:\n",
    "            num_data, img_data, targets = num_data.to(device, non_blocking=True), img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(num_data, img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for num_data, img_data, targets in val_loader:\n",
    "                num_data, img_data, targets = num_data.to(device, non_blocking=True), img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                outputs = model(num_data, img_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "        \n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "        train_mse = mean_squared_error(train_targets, train_predictions)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        val_mse = mean_squared_error(val_targets, val_predictions)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        train_r2 = r2_score(train_targets, train_predictions)\n",
    "        val_r2 = r2_score(val_targets, val_predictions)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "        history['val_mse'].append(val_mse)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        pbar.set_description(\"| train_mse: %.2e | val_mse: %.2e | \" % (train_mse, val_mse))\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    train_metrics = calculate_metrics(model, train_loader, device)\n",
    "    val_metrics = calculate_metrics(model, val_loader, device)\n",
    "    test_metrics = calculate_metrics(model, test_loader, device)\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'train_mse': train_metrics['mse'],\n",
    "        'train_mae': train_metrics['mae'],\n",
    "        'train_rmse': train_metrics['rmse'],\n",
    "        'train_r2': train_metrics['r2'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_mse': val_metrics['mse'],\n",
    "        'val_mae': val_metrics['mae'],\n",
    "        'val_rmse': val_metrics['rmse'],\n",
    "        'val_r2': val_metrics['r2'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_mse': test_metrics['mse'],\n",
    "        'test_mae': test_metrics['mae'],\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'test_r2': test_metrics['r2'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "    print(f\"Best model found at epoch {best_epoch}/{epochs}\")\n",
    "    print(f\"Best Train Loss: {history['train_loss'][best_epoch-1]:.4f}, Best Val Loss: {history['val_loss'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train MSE: {history['train_mse'][best_epoch-1]:.4f}, Best Val MSE: {history['val_mse'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train RMSE: {history['train_rmse'][best_epoch-1]:.4f}, Best Val RMSE: {history['val_rmse'][best_epoch-1]:.4f}\")\n",
    "\n",
    "    # Save metrics to a file\n",
    "    os.makedirs(f'logs/Regression/{dataset_name}/ViT+MLP/{model_name}', exist_ok=True)\n",
    "    with open(f'logs/Regression/{dataset_name}/ViT+MLP/{model_name}/metrics.txt', 'w') as f:\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "            \n",
    "    # Save best model\n",
    "    model_save_path = f\"models/Regression/{dataset_name}/ViT+MLP/{model_name}/best_model.pth\"\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(best_model, model_save_path)\n",
    "    print(f\"Best model saved to {model_save_path}\")\n",
    "            \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def calculate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for num_data, img_data, targets in data_loader:\n",
    "            num_data, img_data, targets = num_data.to(device, non_blocking=True), img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(num_data, img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "    mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_predictions)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9cc7155-a53f-4398-82db-80ecc00fa4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 4.43e+05 | val_loss: 4.35e+05 | val_rmse: 7.07e+02 | : 100%|█| 5/5 [01:07<00:00, 13.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 67.18 seconds\n",
      "Best model found at epoch 5/5\n",
      "Best Train Loss: 443002.1075, Best Val Loss: 434963.9277\n",
      "Best Train MSE: 419667.5000, Best Val MSE: 499989.4375\n",
      "Best Train RMSE: 647.8175, Best Val RMSE: 707.0993\n",
      "Best model saved to models/Regression/1000cameras/ViT+MLP/1000cameras_Model1/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "metrics = compile_and_fit_2(model1, train_loader, val_loader, test_loader, dataset_name, f\"{dataset_name}_Model1\", batch_size=32, epochs=5, min_lr=1e-4, max_lr=4e-3 , device='cuda', weight_decay=1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
